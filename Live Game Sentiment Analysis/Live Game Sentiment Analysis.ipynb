{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KiranH1007/-MLB-fan-engagement-project/blob/load-data-using-pandas/Live%20Game%20Sentiment%20Analysis/Live%20Game%20Sentiment%20Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "Gydyrk4uSbZw"
      },
      "outputs": [],
      "source": [
        "# Authenticate Google Cloud in Colab\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install Required Libraries\n",
        "!pip install google-cloud-storage google-cloud-bigquery nltk pandas"
      ],
      "metadata": {
        "id": "D7NXnwDgSmey"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "from google.cloud import storage\n",
        "\n",
        "# Initialize GCS client\n",
        "storage_client = storage.Client()\n",
        "\n",
        "# Specify your GCS bucket and file name\n",
        "bucket_name = 'gcp-mlb-hackathon-2025' # Make sure this is the correct bucket name\n",
        "blob_name = 'datasets/mlb-fan-content-interaction-data/2025-mlb-fan-favs-follows.json'\n",
        "\n",
        "# Get the blob directly without getting the bucket metadata first\n",
        "# This assumes the bucket is public and blob_name includes the full path\n",
        "blob = storage_client.bucket(bucket_name).blob(blob_name)\n",
        "\n",
        "# Download the file\n",
        "raw_data = blob.download_as_text()\n",
        "\n",
        "# Print the data (optional)\n",
        "#print(raw_data[0:100])"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "3VAZpFJNTBkj"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "source": [
        "# load json with pandas\n",
        "# Data set is 2025-mlb-fan-favs-follows.json\n",
        "import pandas as pd\n",
        "import json\n",
        "from google.cloud import storage\n",
        "\n",
        "# Initialize GCS client\n",
        "storage_client = storage.Client()\n",
        "\n",
        "# Specify your GCS bucket and file name\n",
        "bucket_name = 'gcp-mlb-hackathon-2025'\n",
        "blob_name = 'datasets/mlb-fan-content-interaction-data/2025-mlb-fan-favs-follows.json'\n",
        "\n",
        "# Get the blob directly\n",
        "blob = storage_client.bucket(bucket_name).blob(blob_name)\n",
        "\n",
        "# Download the file as bytes\n",
        "data = blob.download_as_bytes()\n",
        "\n",
        "# Decode the bytes to a string\n",
        "raw_data = data.decode('utf-8')\n",
        "\n",
        "# Load the data using json.loads to handle possible JSON Lines format\n",
        "try:\n",
        "    # Attempt to load as a single JSON object\n",
        "    json_data = json.loads(raw_data)\n",
        "    df = pd.DataFrame([json_data]) # Enclose in a list if it's a single object\n",
        "except json.JSONDecodeError:\n",
        "    # If single object fails, try loading as JSON Lines\n",
        "    json_data = [json.loads(line) for line in raw_data.splitlines() if line.strip()]\n",
        "    df = pd.DataFrame(json_data)\n",
        "\n",
        "# Preview data\n",
        "print(df.head())\n",
        "\n",
        "print(df.info())"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "8Gg8bw6lf4WK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load json with pandas\n",
        "# data set is datasets_mlb-fan-content-interaction-data_2025-mlb-fan-favs-follows-000000000000.json\n",
        "import pandas as pd\n",
        "import json\n",
        "from google.cloud import storage\n",
        "\n",
        "# Initialize GCS client\n",
        "storage_client = storage.Client()\n",
        "\n",
        "# Specify your GCS bucket and file name\n",
        "bucket_name = 'gcp-mlb-hackathon-2025'\n",
        "blob_name = 'datasets/mlb-fan-content-interaction-data/datasets_mlb-fan-content-interaction-data_2025-mlb-fan-favs-follows-000000000000.json'\n",
        "\n",
        "# Get the blob directly\n",
        "blob = storage_client.bucket(bucket_name).blob(blob_name)\n",
        "\n",
        "# Download the file as bytes\n",
        "data = blob.download_as_bytes()\n",
        "\n",
        "# Decode the bytes to a string\n",
        "raw_data = data.decode('utf-8')\n",
        "\n",
        "# Load the data using json.loads to handle possible JSON Lines format\n",
        "try:\n",
        "    # Attempt to load as a single JSON object\n",
        "    json_data = json.loads(raw_data)\n",
        "    df2 = pd.DataFrame([json_data]) # Enclose in a list if it's a single object\n",
        "except json.JSONDecodeError:\n",
        "    # If single object fails, try loading as JSON Lines\n",
        "    json_data = [json.loads(line) for line in raw_data.splitlines() if line.strip()]\n",
        "    df2 = pd.DataFrame(json_data)\n",
        "\n",
        "# Preview data\n",
        "print(df2.head())\n",
        "\n",
        "print(df2.info())\n"
      ],
      "metadata": {
        "id": "vL0cEBsG_zgr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load json with pandas\n",
        "# data set is mlb-fan-content-interaction-data-000000000000.json\n",
        "import pandas as pd\n",
        "import json\n",
        "from google.cloud import storage\n",
        "\n",
        "# Initialize GCS client\n",
        "storage_client = storage.Client()\n",
        "\n",
        "# Specify your GCS bucket and file name\n",
        "bucket_name = 'gcp-mlb-hackathon-2025'\n",
        "blob_name = 'datasets/mlb-fan-content-interaction-data/mlb-fan-content-interaction-data-000000000000.json'\n",
        "\n",
        "# Get the blob directly\n",
        "blob = storage_client.bucket(bucket_name).blob(blob_name)\n",
        "\n",
        "# Download the file as bytes\n",
        "data = blob.download_as_bytes()\n",
        "\n",
        "# Decode the bytes to a string\n",
        "raw_data = data.decode('utf-8')\n",
        "\n",
        "# Load the data using json.loads to handle possible JSON Lines format\n",
        "try:\n",
        "    # Attempt to load as a single JSON object\n",
        "    json_data = json.loads(raw_data)\n",
        "    df3 = pd.DataFrame([json_data]) # Enclose in a list if it's a single object\n",
        "except json.JSONDecodeError:\n",
        "    # If single object fails, try loading as JSON Lines\n",
        "    json_data = [json.loads(line) for line in raw_data.splitlines() if line.strip()]\n",
        "    df3 = pd.DataFrame(json_data)\n",
        "\n",
        "# make single column for all content to content id due to similar data like slug and content_headline\n",
        "'''\n",
        "Redundancy: Storing essentially the same information twice.\n",
        "Increased storage: Larger data size due to duplicated information.\n",
        "Potential inconsistency: If one field is updated and the other isn't, it could create discrepancies.\n",
        "'''\n",
        "#step 1 observation\n",
        "# print(len(df3['slug']))\n",
        "# print(len(df3['content_headline']))\n",
        "# print(df3['slug'][100])\n",
        "# print(df3['content_headline'][100])\n",
        "# print(df3['content_type'][100])\n",
        "\n",
        "# step 2 combining\n",
        "'''\n",
        "Prioritize One Field:\n",
        "If slug and content_headline convey nearly identical information, choose the one that's more concise and\n",
        "consistently formatted (likely slug) as the primary identifier.\n",
        "In most cases, the slug is a better choice for the primary identifier because:\n",
        "Conciseness: Slugs are typically shorter and more URL-friendly than content headlines.\n",
        "Consistency: Slugs are often generated programmatically and have a more consistent format compared to headlines, which might contain variations in punctuation and capitalization.\n",
        "'''\n",
        "# remove extra column\n",
        "df3.drop(columns=['content_headline'], inplace=True)\n",
        "df3['content_id'] = df3['content_type'] + ':' + df3['slug'].str.lower()\n",
        "df3.drop(columns=['slug'], inplace=True)\n",
        "df3.drop(columns=['content_type'], inplace=True)\n",
        "\n",
        "#print(df3['content_id'][100])\n",
        "\n",
        "\n",
        "# similarly for date time utc as date_time_date is same as utc\n",
        "df3.drop(columns=['date_time_date'], inplace=True) # Drop the redundant column\n",
        "\n",
        "# Preview data\n",
        "df3.head()\n",
        "\n",
        "#print(df3.info())"
      ],
      "metadata": {
        "id": "5s0D9_vr9zu9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# Analysis of DataFrames are named df, df2, df3\n",
        "common_columns = list(set(df.columns) & set(df2.columns) & set(df3.columns))\n",
        "print(\"Common Columns:\", common_columns)\n"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "_uHfGTksBxcN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#without cleanup and duplicate columns\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# it should be outer for all unique data and inner for common data\n",
        "\n",
        "# 1. Merge the first two datasets\n",
        "merged_df = pd.merge(df, df2, on=common_columns, how='outer')\n",
        "\n",
        "# 2. Merge the result with the third dataset which has content type headline\n",
        "merged_df = pd.merge(merged_df, df3, on=common_columns, how='outer')\n",
        "\n",
        "merged_df.count()\n",
        "\n",
        "merged_df.head()"
      ],
      "metadata": {
        "id": "a7DX81VVC9HT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# with cleanup and removal of duplicate columns\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "def merge_and_handle_duplicates_concise(df1, df2, on_columns):\n",
        "    merged_df = pd.merge(df1, df2, on=on_columns, how='outer')\n",
        "    for col in [c for c in merged_df if any(c.endswith(s) for s in ('_x', '_y'))]:\n",
        "        other_col = col[:-2] + ('_y' if col.endswith('_x') else '_x')\n",
        "        if other_col in merged_df:\n",
        "            # Check if values are the same and have the same data type\n",
        "            if merged_df[col].equals(merged_df[other_col]):\n",
        "\n",
        "                # If same, keep one and drop the other\n",
        "                merged_df = merged_df.drop(columns=[other_col])\n",
        "                merged_df = merged_df.rename(columns={col: col[:-2]})  # Rename to remove suffix\n",
        "            else:\n",
        "                # If different (list type or other differences), apply previous logic\n",
        "                if merged_df[col].apply(isinstance, args=(list,)).any() and merged_df[other_col].apply(isinstance, args=(list,)).any():\n",
        "                    for c in (col, other_col):\n",
        "                        merged_df[c] = merged_df[c].apply(lambda x: sorted(x) if isinstance(x, list) else x)\n",
        "                merged_df[col[:-2]] = merged_df[col]  # Or apply a custom logic here\n",
        "                merged_df = merged_df.drop(columns=[other_col])\n",
        "    return merged_df\n",
        "\n",
        "merged_df = merge_and_handle_jumbled_lists(df, df2, common_columns)\n",
        "\n",
        "# removed manually as it has same values in favorite_team_id_x\n",
        "merged_df = merged_df.drop(columns=['favorite_team_id_y'])\n",
        "\n",
        "merged_df = pd.merge(merged_df, df3, on=common_columns, how='outer')\n",
        "\n",
        "\n",
        "# BEFORE CLEANUPS stats\n",
        "print(merged_df.isnull().sum())\n",
        "\n",
        "# Remove rows with missing values in user_id\n",
        "#merged_df.dropna(subset=['user_id'], inplace=True)\n",
        "\n",
        "# Remove duplicates based on user_id keep only unique users\n",
        "#merged_df.drop_duplicates(subset=['user_id'], inplace=True)\n",
        "#print(merged_df.isnull().sum())\n",
        "\n",
        "# Preview data\n",
        "merged_df.head()\n",
        "\n",
        "# defines column names and datatypes\n",
        "#merged_df.info()"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "K51FrtgXpgMq"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}