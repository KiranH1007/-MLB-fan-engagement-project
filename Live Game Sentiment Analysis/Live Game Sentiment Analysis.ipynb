{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KiranH1007/-MLB-fan-engagement-project/blob/build-fan-engagement-application-using-ai-models/Live%20Game%20Sentiment%20Analysis/Live%20Game%20Sentiment%20Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "Gydyrk4uSbZw"
      },
      "outputs": [],
      "source": [
        "# Authenticate Google Cloud in Colab\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install Required Libraries\n",
        "!pip install google-cloud-storage google-cloud-bigquery nltk pandas"
      ],
      "metadata": {
        "id": "D7NXnwDgSmey"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "from google.cloud import storage\n",
        "\n",
        "# Initialize GCS client\n",
        "storage_client = storage.Client()\n",
        "\n",
        "# Specify your GCS bucket and file name\n",
        "bucket_name = 'gcp-mlb-hackathon-2025' # Make sure this is the correct bucket name\n",
        "blob_name = 'datasets/mlb-fan-content-interaction-data/2025-mlb-fan-favs-follows.json'\n",
        "\n",
        "# Get the blob directly without getting the bucket metadata first\n",
        "# This assumes the bucket is public and blob_name includes the full path\n",
        "blob = storage_client.bucket(bucket_name).blob(blob_name)\n",
        "\n",
        "# Download the file\n",
        "raw_data = blob.download_as_text()\n",
        "\n",
        "# Print the data (optional)\n",
        "#print(raw_data[0:100])"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "3VAZpFJNTBkj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# load json with pandas\n",
        "# Data set is 2025-mlb-fan-favs-follows.json\n",
        "import pandas as pd\n",
        "import json\n",
        "from google.cloud import storage\n",
        "\n",
        "# Initialize GCS client\n",
        "storage_client = storage.Client()\n",
        "\n",
        "# Specify your GCS bucket and file name\n",
        "bucket_name = 'gcp-mlb-hackathon-2025'\n",
        "blob_name = 'datasets/mlb-fan-content-interaction-data/2025-mlb-fan-favs-follows.json'\n",
        "\n",
        "# Get the blob directly\n",
        "blob = storage_client.bucket(bucket_name).blob(blob_name)\n",
        "\n",
        "# Download the file as bytes\n",
        "data = blob.download_as_bytes()\n",
        "\n",
        "# Decode the bytes to a string\n",
        "raw_data = data.decode('utf-8')\n",
        "\n",
        "# Load the data using json.loads to handle possible JSON Lines format\n",
        "try:\n",
        "    # Attempt to load as a single JSON object\n",
        "    json_data = json.loads(raw_data)\n",
        "    df_fanfav = pd.DataFrame([json_data]) # Enclose in a list if it's a single object\n",
        "except json.JSONDecodeError:\n",
        "    # If single object fails, try loading as JSON Lines\n",
        "    json_data = [json.loads(line) for line in raw_data.splitlines() if line.strip()]\n",
        "    df_fanfav = pd.DataFrame(json_data)\n",
        "\n",
        "# Preview data\n",
        "#print(df_fanfav.head())\n",
        "\n",
        "print(df_fanfav.info())"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "8Gg8bw6lf4WK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load json with pandas\n",
        "# data set is mlb-fan-content-interaction-data-000000000000.json\n",
        "\n",
        "'''  sample code for analysis of one dataset\n",
        "import pandas as pd\n",
        "import json\n",
        "from google.cloud import storage\n",
        "\n",
        "# Initialize GCS client\n",
        "storage_client = storage.Client()\n",
        "\n",
        "# Specify your GCS bucket and file name\n",
        "bucket_name = 'gcp-mlb-hackathon-2025'\n",
        "blob_name = 'datasets/mlb-fan-content-interaction-data/mlb-fan-content-interaction-data-000000000000.json'\n",
        "\n",
        "# Get the blob directly\n",
        "blob = storage_client.bucket(bucket_name).blob(blob_name)\n",
        "\n",
        "# Download the file as bytes\n",
        "data = blob.download_as_bytes()\n",
        "\n",
        "# Decode the bytes to a string\n",
        "raw_data = data.decode('utf-8')\n",
        "\n",
        "# Load the data using json.loads to handle possible JSON Lines format\n",
        "try:\n",
        "    # Attempt to load as a single JSON object\n",
        "    json_data = json.loads(raw_data)\n",
        "    df3 = pd.DataFrame([json_data]) # Enclose in a list if it's a single object\n",
        "except json.JSONDecodeError:\n",
        "    # If single object fails, try loading as JSON Lines\n",
        "    json_data = [json.loads(line) for line in raw_data.splitlines() if line.strip()]\n",
        "    df3 = pd.DataFrame(json_data)\n",
        "\n",
        "# make single column for all content to content id due to similar data like slug and content_headline\n",
        "#\n",
        "# Redundancy: Storing essentially the same information twice.\n",
        "# Increased storage: Larger data size due to duplicated information.\n",
        "# Potential inconsistency: If one field is updated and the other isn't, it could create discrepancies.\n",
        "#\n",
        "#step 1 observation\n",
        "# print(len(df3['slug']))\n",
        "# print(len(df3['content_headline']))\n",
        "# print(df3['slug'][100])\n",
        "# print(df3['content_headline'][100])\n",
        "# print(df3['content_type'][100])\n",
        "\n",
        "# step 2 combining\n",
        "# Prioritize One Field:\n",
        "# If slug and content_headline convey nearly identical information, choose the one that's more concise and\n",
        "# consistently formatted (likely slug) as the primary identifier.\n",
        "# In most cases, the slug is a better choice for the primary identifier because:\n",
        "# Conciseness: Slugs are typically shorter and more URL-friendly than content headlines.\n",
        "# Consistency: Slugs are often generated programmatically and have a more consistent format compared to headlines, which might contain variations in punctuation and capitalization.\n",
        "\n",
        "# remove extra column\n",
        "df3.drop(columns=['content_headline'], inplace=True)\n",
        "df3['content_id'] = df3['content_type'] + ':' + df3['slug'].str.lower()\n",
        "df3.drop(columns=['slug'], inplace=True)\n",
        "df3.drop(columns=['content_type'], inplace=True)\n",
        "\n",
        "#print(df3['content_id'][100])\n",
        "\n",
        "\n",
        "# similarly for date time utc as date_time_date is same as utc\n",
        "df3.drop(columns=['date_time_date'], inplace=True) # Drop the redundant column\n",
        "\n",
        "# Preview data\n",
        "df3.head()\n",
        "\n",
        "#print(df3.info())\n",
        "\n",
        "'''"
      ],
      "metadata": {
        "id": "5s0D9_vr9zu9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load json with pandas\n",
        "# data set altogether with  multiple json files\n",
        "# Note ******** currently doing with 5 files later do the Scalable\n",
        "import pandas as pd\n",
        "import json\n",
        "from google.cloud import storage\n",
        "import re\n",
        "\n",
        "# Initialize GCS client\n",
        "storage_client = storage.Client()\n",
        "\n",
        "# Specify your GCS bucket and file name\n",
        "bucket_name = 'gcp-mlb-hackathon-2025'\n",
        "file_pattern = 'datasets/mlb-fan-content-interaction-data/'\n",
        "\n",
        "# List to store dataframes from each file\n",
        "all_dfs = []\n",
        "\n",
        "blobs = storage_client.list_blobs(bucket_name, prefix=file_pattern)\n",
        "# for blob in blobs:\n",
        "#     print(blob.name)\n",
        "\n",
        "filtered_blobs = [blob for blob in blobs if re.match(r'datasets/mlb-fan-content-interaction-data/mlb-fan-content-interaction-data-\\d+\\.json$', blob.name)]\n",
        "\n",
        "# limiting the dataset\n",
        "filtered_blobs = filtered_blobs[:5]\n",
        "\n",
        "for blob_name in filtered_blobs:\n",
        "\n",
        "       print(f\"Processing file: {blob_name.name}\")\n",
        "       # Get the blob directly\n",
        "       blob = storage_client.bucket(bucket_name).blob(blob_name.name)\n",
        "\n",
        "       # Download the file as bytes\n",
        "       data = blob.download_as_bytes()\n",
        "\n",
        "       # Decode the bytes to a string\n",
        "       raw_data = data.decode('utf-8')\n",
        "\n",
        "       # Load the data using json.loads\n",
        "       try:\n",
        "           json_data = json.loads(raw_data)\n",
        "           df = pd.DataFrame([json_data])\n",
        "       except json.JSONDecodeError:\n",
        "           json_data = [json.loads(line) for line in raw_data.splitlines() if line.strip()]\n",
        "           df = pd.DataFrame(json_data)\n",
        "\n",
        "       #print(df.head())\n",
        "       # Data Cleaning: Remove Redundancy (similar to your example)\n",
        "       df.drop(columns=['content_headline'], inplace=True, errors='ignore')  # Ignore if column doesn't exist\n",
        "       df['content_id'] = df['content_type'] + ':' + df['slug'].str.lower()\n",
        "       df.drop(columns=['slug', 'content_type'], inplace=True, errors='ignore')\n",
        "       df.drop(columns=['date_time_date'], inplace=True, errors='ignore')\n",
        "\n",
        "       # Add the cleaned dataframe to the list\n",
        "       all_dfs.append(df)\n",
        "\n",
        "final_df = pd.concat(all_dfs, ignore_index=True)\n",
        "\n",
        "final_df.head()"
      ],
      "metadata": {
        "id": "z767Md64tgus"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# Analysis of DataFrames are named df, df2, df3\n",
        "common_columns = list(set(df_fanfav.columns) & set(final_df.columns))\n",
        "print(\"Common Columns:\", common_columns)\n"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "_uHfGTksBxcN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#without cleanup and duplicate columns\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# it should be outer for all unique data and inner for common data\n",
        "\n",
        "# Merge the result with the third dataset which has content type headline\n",
        "merged_df = pd.merge(df_fanfav, final_df, on=common_columns, how='outer')\n",
        "\n",
        "print(merged_df.count())\n",
        "\n",
        "merged_df.head()"
      ],
      "metadata": {
        "id": "a7DX81VVC9HT"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}