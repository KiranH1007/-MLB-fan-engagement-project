{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KiranH1007/-MLB-fan-engagement-project/blob/build-fan-engagement-application-using-ai-models/Live%20Game%20Sentiment%20Analysis/Live%20Game%20Sentiment%20Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "Gydyrk4uSbZw"
      },
      "outputs": [],
      "source": [
        "# Authenticate Google Cloud in Colab\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install Required Libraries\n",
        "!pip install google-cloud-storage google-cloud-bigquery nltk pandas"
      ],
      "metadata": {
        "id": "D7NXnwDgSmey"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "from google.cloud import storage\n",
        "\n",
        "# Initialize GCS client\n",
        "storage_client = storage.Client()\n",
        "\n",
        "# Specify your GCS bucket and file name\n",
        "bucket_name = 'gcp-mlb-hackathon-2025' # Make sure this is the correct bucket name\n",
        "blob_name = 'datasets/mlb-fan-content-interaction-data/2025-mlb-fan-favs-follows.json'\n",
        "\n",
        "# Get the blob directly without getting the bucket metadata first\n",
        "# This assumes the bucket is public and blob_name includes the full path\n",
        "blob = storage_client.bucket(bucket_name).blob(blob_name)\n",
        "\n",
        "# Download the file\n",
        "raw_data = blob.download_as_text()\n",
        "\n",
        "# Print the data (optional)\n",
        "#print(raw_data[0:100])"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "3VAZpFJNTBkj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# load json with pandas\n",
        "# Data set is 2025-mlb-fan-favs-follows.json\n",
        "import pandas as pd\n",
        "import json\n",
        "from google.cloud import storage\n",
        "\n",
        "# Initialize GCS client\n",
        "storage_client = storage.Client()\n",
        "\n",
        "# Specify your GCS bucket and file name\n",
        "bucket_name = 'gcp-mlb-hackathon-2025'\n",
        "blob_name = 'datasets/mlb-fan-content-interaction-data/2025-mlb-fan-favs-follows.json'\n",
        "\n",
        "# Get the blob directly\n",
        "blob = storage_client.bucket(bucket_name).blob(blob_name)\n",
        "\n",
        "# Download the file as bytes\n",
        "data = blob.download_as_bytes()\n",
        "\n",
        "# Decode the bytes to a string\n",
        "raw_data = data.decode('utf-8')\n",
        "\n",
        "# Load the data using json.loads to handle possible JSON Lines format\n",
        "try:\n",
        "    # Attempt to load as a single JSON object\n",
        "    json_data = json.loads(raw_data)\n",
        "    df_fanfav = pd.DataFrame([json_data]) # Enclose in a list if it's a single object\n",
        "except json.JSONDecodeError:\n",
        "    # If single object fails, try loading as JSON Lines\n",
        "    json_data = [json.loads(line) for line in raw_data.splitlines() if line.strip()]\n",
        "    df_fanfav = pd.DataFrame(json_data)\n",
        "\n",
        "# Preview data\n",
        "print(df_fanfav.head())\n",
        "\n",
        "print(df_fanfav.info())"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "8Gg8bw6lf4WK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load json with pandas\n",
        "# data set is mlb-fan-content-interaction-data-000000000000.json\n",
        "\n",
        "'''  sample code for analysis of one dataset\n",
        "import pandas as pd\n",
        "import json\n",
        "from google.cloud import storage\n",
        "\n",
        "# Initialize GCS client\n",
        "storage_client = storage.Client()\n",
        "\n",
        "# Specify your GCS bucket and file name\n",
        "bucket_name = 'gcp-mlb-hackathon-2025'\n",
        "blob_name = 'datasets/mlb-fan-content-interaction-data/mlb-fan-content-interaction-data-000000000000.json'\n",
        "\n",
        "# Get the blob directly\n",
        "blob = storage_client.bucket(bucket_name).blob(blob_name)\n",
        "\n",
        "# Download the file as bytes\n",
        "data = blob.download_as_bytes()\n",
        "\n",
        "# Decode the bytes to a string\n",
        "raw_data = data.decode('utf-8')\n",
        "\n",
        "# Load the data using json.loads to handle possible JSON Lines format\n",
        "try:\n",
        "    # Attempt to load as a single JSON object\n",
        "    json_data = json.loads(raw_data)\n",
        "    df3 = pd.DataFrame([json_data]) # Enclose in a list if it's a single object\n",
        "except json.JSONDecodeError:\n",
        "    # If single object fails, try loading as JSON Lines\n",
        "    json_data = [json.loads(line) for line in raw_data.splitlines() if line.strip()]\n",
        "    df3 = pd.DataFrame(json_data)\n",
        "\n",
        "# make single column for all content to content id due to similar data like slug and content_headline\n",
        "#\n",
        "# Redundancy: Storing essentially the same information twice.\n",
        "# Increased storage: Larger data size due to duplicated information.\n",
        "# Potential inconsistency: If one field is updated and the other isn't, it could create discrepancies.\n",
        "#\n",
        "#step 1 observation\n",
        "# print(len(df3['slug']))\n",
        "# print(len(df3['content_headline']))\n",
        "# print(df3['slug'][100])\n",
        "# print(df3['content_headline'][100])\n",
        "# print(df3['content_type'][100])\n",
        "\n",
        "# step 2 combining\n",
        "# Prioritize One Field:\n",
        "# If slug and content_headline convey nearly identical information, choose the one that's more concise and\n",
        "# consistently formatted (likely slug) as the primary identifier.\n",
        "# In most cases, the slug is a better choice for the primary identifier because:\n",
        "# Conciseness: Slugs are typically shorter and more URL-friendly than content headlines.\n",
        "# Consistency: Slugs are often generated programmatically and have a more consistent format compared to headlines, which might contain variations in punctuation and capitalization.\n",
        "\n",
        "# remove extra column\n",
        "df3.drop(columns=['content_headline'], inplace=True)\n",
        "df3['content_id'] = df3['content_type'] + ':' + df3['slug'].str.lower()\n",
        "df3.drop(columns=['slug'], inplace=True)\n",
        "df3.drop(columns=['content_type'], inplace=True)\n",
        "\n",
        "#print(df3['content_id'][100])\n",
        "\n",
        "\n",
        "# similarly for date time utc as date_time_date is same as utc\n",
        "df3.drop(columns=['date_time_date'], inplace=True) # Drop the redundant column\n",
        "\n",
        "# Preview data\n",
        "df3.head()\n",
        "\n",
        "#print(df3.info())\n",
        "\n",
        "'''"
      ],
      "metadata": {
        "id": "5s0D9_vr9zu9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load json with pandas\n",
        "# data set altogether with  multiple json files\n",
        "# Note ******** currently doing with 5 files later do the Scalable\n",
        "import pandas as pd\n",
        "import json\n",
        "from google.cloud import storage\n",
        "import re\n",
        "\n",
        "# Initialize GCS client\n",
        "storage_client = storage.Client()\n",
        "\n",
        "# Specify your GCS bucket and file name\n",
        "bucket_name = 'gcp-mlb-hackathon-2025'\n",
        "file_pattern = 'datasets/mlb-fan-content-interaction-data/'\n",
        "\n",
        "# List to store dataframes from each file\n",
        "all_dfs = []\n",
        "\n",
        "blobs = storage_client.list_blobs(bucket_name, prefix=file_pattern)\n",
        "# for blob in blobs:\n",
        "#     print(blob.name)\n",
        "\n",
        "filtered_blobs = [blob for blob in blobs if re.match(r'datasets/mlb-fan-content-interaction-data/mlb-fan-content-interaction-data-\\d+\\.json$', blob.name)]\n",
        "\n",
        "# limiting the dataset\n",
        "filtered_blobs = filtered_blobs[:5]\n",
        "\n",
        "for blob_name in filtered_blobs:\n",
        "\n",
        "       print(f\"Processing file: {blob_name.name}\")\n",
        "       # Get the blob directly\n",
        "       blob = storage_client.bucket(bucket_name).blob(blob_name.name)\n",
        "\n",
        "       # Download the file as bytes\n",
        "       data = blob.download_as_bytes()\n",
        "\n",
        "       # Decode the bytes to a string\n",
        "       raw_data = data.decode('utf-8')\n",
        "\n",
        "       # Load the data using json.loads\n",
        "       try:\n",
        "           json_data = json.loads(raw_data)\n",
        "           df = pd.DataFrame([json_data])\n",
        "       except json.JSONDecodeError:\n",
        "           json_data = [json.loads(line) for line in raw_data.splitlines() if line.strip()]\n",
        "           df = pd.DataFrame(json_data)\n",
        "\n",
        "       #print(df.head())\n",
        "       # Data Cleaning: Remove Redundancy (similar to your example)\n",
        "       df.drop(columns=['content_headline'], inplace=True, errors='ignore')  # Ignore if column doesn't exist\n",
        "       df['content_id'] = df['content_type'] + ':' + df['slug'].str.lower()\n",
        "       df.drop(columns=['slug', 'content_type'], inplace=True, errors='ignore')\n",
        "       df.drop(columns=['date_time_date'], inplace=True, errors='ignore')\n",
        "\n",
        "       # Add the cleaned dataframe to the list\n",
        "       all_dfs.append(df)\n",
        "\n",
        "final_df = pd.concat(all_dfs, ignore_index=True)\n",
        "\n",
        "final_df.head()"
      ],
      "metadata": {
        "id": "z767Md64tgus"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# Analysis of DataFrames are named df_fanfav, final_df\n",
        "common_columns = list(set(df_fanfav.columns) & set(final_df.columns))\n",
        "print(\"Common Columns:\", common_columns)\n"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "_uHfGTksBxcN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#without cleanup and duplicate columns\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# it should be outer for all unique data and inner for common data\n",
        "\n",
        "# Merge the result with the third dataset which has content type headline\n",
        "merged_df = pd.merge(df_fanfav, final_df, on=common_columns, how='outer')\n",
        "\n",
        "print(merged_df.count())\n",
        "\n",
        "merged_df.head()"
      ],
      "metadata": {
        "id": "a7DX81VVC9HT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MY-kFSSVu0B2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "import pickle\n",
        "with open('merged_df.pkl', 'wb') as f:\n",
        "   pickle.dump(merged_df, f)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "XN-AnenyS_ca"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "checkpoint saved for merged_df dataset\n"
      ],
      "metadata": {
        "id": "p6VX8ZBjT46c"
      }
    },
    {
      "source": [
        "with open('merged_df.pkl', 'rb') as f:\n",
        "   merged_df = pickle.load(f)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "O47cNh4GT35p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "user_row = merged_df[merged_df['user_id'] == 'some_user_id']\n",
        "print(user_row)"
      ],
      "metadata": {
        "id": "5VDwVVjbWPHR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fan Engagement Recommendation System using only in context with user id who followed  followed_player_ids column and player_tags based on content id column.\n",
        "1. Overview\n",
        "\n",
        "This system provides personalized content recommendations to fans based on their followed players and popular content. It leverages a dataset called merged_df, which contains information about users, their followed players, and content with associated player tags.\n",
        "\n",
        "2. Data Structure\n",
        "\n",
        "The system relies on the following data structures:\n",
        "\n",
        "user_player_mapping: A dictionary that maps each user ID to a set of player IDs they follow. This mapping is derived from the followed_player_ids column in merged_df.\n",
        "player_content_mapping: A dictionary that maps each player tag to a set of content IDs associated with that tag. This mapping is created using the player_tags and content_id columns in merged_df.\n",
        "3. Recommendation Logic\n",
        "\n",
        "The core recommendation logic is implemented in the recommend_content function:\n",
        "\n",
        "Input: The function takes a user_id and top_n (number of recommendations) as input.\n",
        "Player-Based Recommendations: If the user_id is found in user_player_mapping, it iterates through the followed players and retrieves associated content from player_content_mapping. These recommendations are added to a set called recommendations.\n",
        "Fallback: If no recommendations are found based on followed players (e.g., for new users), a fallback strategy is used. In this case, it recommends the most popular content from merged_df based on the content_id column.\n",
        "Output: The function returns a list of up to top_n recommended content IDs.\n",
        "4. Example Usage\n",
        "\n",
        "\n",
        "user_recommendations = recommend_content('some_user_id', top_n=5)\n",
        "for i in user_recommendations:\n",
        "    print(i)\n",
        "Use code with caution\n",
        "This code snippet demonstrates how to get recommendations for a specific user and print them.\n",
        "\n"
      ],
      "metadata": {
        "id": "B9GeJa0gYJfY"
      }
    },
    {
      "source": [
        "# first sample takes more time\n",
        "# import pandas as pd\n",
        "# from collections import defaultdict\n",
        "\n",
        "# # 1. Build User-Player and Player-Content Mappings\n",
        "# user_player_mapping = defaultdict(set)\n",
        "# player_content_mapping = defaultdict(set)\n",
        "\n",
        "# for _, row in merged_df.iterrows():\n",
        "#     user_id = row['user_id']\n",
        "#     followed_players = row['followed_player_ids']\n",
        "#     content_id = row['content_id']\n",
        "#     player_tags = row['player_tags']\n",
        "\n",
        "#     if isinstance(followed_players, str):\n",
        "#         for player in followed_players.split(','):\n",
        "#             user_player_mapping[user_id].add(player.strip())\n",
        "\n",
        "#     if isinstance(player_tags, str):\n",
        "#         for tag in player_tags.split(','):\n",
        "#             player_content_mapping[tag.strip()].add(content_id)\n",
        "\n",
        "# print(user_player_mapping)\n",
        "# print(\"hi\")\n",
        "# print(player_content_mapping)\n"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "z3g7ajWQVDlO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert columns to string type, handling NaN values\n",
        "merged_df['followed_player_ids'] = merged_df['followed_player_ids'].astype(str)\n",
        "merged_df['player_tags'] = merged_df['player_tags'].astype(str)"
      ],
      "metadata": {
        "id": "fVIP3MWrhnxV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# optimised way\n",
        "import pandas as pd\n",
        "from collections import defaultdict\n",
        "\n",
        "# 1. Build User-Player Mapping\n",
        "# Explode followed_player_ids and apply a lambda function\n",
        "user_player_df = merged_df.assign(followed_player_ids=merged_df['followed_player_ids'].str.split(',')).explode('followed_player_ids')\n",
        "user_player_df['followed_player_ids'] = user_player_df['followed_player_ids'].str.strip()\n",
        "user_player_mapping1 = user_player_df.groupby('user_id')['followed_player_ids'].apply(set).to_dict()\n",
        "\n",
        "\n",
        "# 2. Build Player-Content Mapping\n",
        "# Explode player_tags and apply a lambda function\n",
        "player_content_df = merged_df.assign(player_tags=merged_df['player_tags'].str.split(',')).explode('player_tags')\n",
        "player_content_df['player_tags'] = player_content_df['player_tags'].str.strip()\n",
        "player_content_mapping1 = player_content_df.groupby('player_tags')['content_id'].apply(set).to_dict()\n"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "edMUr50OhLhC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "import random\n",
        "\n",
        "# Get a list of user IDs from the mapping\n",
        "user_ids = list(user_player_mapping1.keys())\n",
        "\n",
        "# Randomly select 5 user IDs\n",
        "sample_user_ids = random.sample(user_ids, 35)\n",
        "\n",
        "# Print the mappings for the selected user IDs M1RJCXEE8IY1NU5\n",
        "for user_id in sample_user_ids:\n",
        "  print(f\"User ID: {user_id}, Followed Players: {user_player_mapping1[user_id]}\")"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "kPaeQhtWinWp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VkFUWjZ8hK_b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "import random\n",
        "\n",
        "# Get a list of user IDs from the mapping\n",
        "player_ids = list(player_content_mapping1.keys())\n",
        "\n",
        "# Randomly select 5 user IDs\n",
        "#sample_player_ids = random.sample(player_ids,'703420')\n",
        "\n",
        "# Print the mappings for the selected player ids\n",
        "#player_id = {'660271'}\n",
        "\n",
        "for player_id in player_ids:\n",
        "  print(f\"player tags {player_id}, contentid: {player_content_mapping1[player_id]}\")"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "S3IAlR-fjSNt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Recommendation Function\n",
        "def recommend_content(user_id, top_n=10):\n",
        "    recommendations = set()\n",
        "    if user_id in user_player_mapping1:\n",
        "        for player in user_player_mapping1[user_id]:\n",
        "            if player in player_content_mapping1:\n",
        "                recommendations.update(player_content_mapping1[player])\n",
        "                print(player_content_mapping1[player])\n",
        "\n",
        "    # Fallback (if no recommendations based on followed players)\n",
        "    if not recommendations:\n",
        "        print(\"no recommendation\")\n",
        "        # Recommend most popular content or other strategies\n",
        "        popular_content = merged_df['content_id'].value_counts().index.tolist()\n",
        "        recommendations.update(popular_content[:top_n])\n",
        "\n",
        "    return list(recommendations)[:top_n]\n",
        "\n",
        "# Example Usage:\n",
        "user_recommendations = recommend_content('some_user_id', top_n=5)\n",
        "for i in user_recommendations:\n",
        "  print(i)"
      ],
      "metadata": {
        "id": "LMopkVJggaiS"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}